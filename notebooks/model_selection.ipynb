{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1eeeb04-fa84-4ae4-b8d9-cb6a5bbda385",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/mot/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/mot/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/mot/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /home/mot/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Download required NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c965d5-e2dd-4a75-835d-395edf044134",
   "metadata": {},
   "source": [
    "# Text Preprocessing Pipeline\n",
    "\n",
    "## 1. Basic Text Cleaning\n",
    "- Convert to lowercase: `text.lower()`\n",
    "- Tokenization: Split text into individual words\n",
    "- Remove non-alphanumeric tokens: `token.isalnum()`\n",
    "\n",
    "## 2. Language Processing\n",
    "- Remove stopwords (common words like 'the', 'is', 'at')\n",
    "- Lemmatization: Reduce words to base form\n",
    " - running → run\n",
    " - better → good\n",
    " - children → child\n",
    "\n",
    "## 3. TF-IDF Vectorization\n",
    "- TF (Term Frequency): How often word appears in document\n",
    "- IDF (Inverse Document Frequency): Word rarity across documents\n",
    "- Final score = TF * IDF\n",
    "- Captures word importance while accounting for frequency bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6b6a469-2270-481b-9e63-4af2856a2ef0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3088 articles > 8400.0 chars\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 58686 entries, 0 to 127029\n",
      "Data columns (total 7 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   title         58686 non-null  object\n",
      " 1   text          58686 non-null  object\n",
      " 2   fake          58686 non-null  int64 \n",
      " 3   title_clean   58686 non-null  object\n",
      " 4   text_clean    58686 non-null  object\n",
      " 5   title_length  58686 non-null  int64 \n",
      " 6   text_length   58686 non-null  int64 \n",
      "dtypes: int64(3), object(4)\n",
      "memory usage: 3.6+ MB\n"
     ]
    }
   ],
   "source": [
    "def combine_news_datasets(fake_path, true_path, final_en_path, welfake_path):\n",
    "    # Load datasets\n",
    "    fake_df = pd.read_csv(fake_path)\n",
    "    true_df = pd.read_csv(true_path)\n",
    "    final_en_df = pd.read_csv(final_en_path)\n",
    "    welfake_df = pd.read_csv(welfake_path)\n",
    "\n",
    "    # Fix fake labels for each dataset\n",
    "    fake_df['fake'] = 1\n",
    "    true_df['fake'] = 0\n",
    "    final_en_df['fake'] = 1 - final_en_df['lebel']  # Convert 0=fake to 1=fake\n",
    "    welfake_df['fake'] = welfake_df['label']\n",
    "\n",
    "    # Select required columns and combine\n",
    "    dfs = []\n",
    "    for df in [fake_df, true_df, final_en_df, welfake_df]:\n",
    "        df = df[['title', 'text', 'fake']].copy()\n",
    "        dfs.append(df)\n",
    "    \n",
    "    combined_df = pd.concat(dfs, ignore_index=True)\n",
    "    \n",
    "    # Clean data\n",
    "    combined_df['title'] = combined_df['title'].fillna('').str.strip()\n",
    "    combined_df['text'] = combined_df['text'].fillna('').str.strip()\n",
    "    \n",
    "    # Drop rows with empty title or text\n",
    "    combined_df = combined_df[\n",
    "        (combined_df['title'] != '') & \n",
    "        (combined_df['text'] != '') &\n",
    "        (~combined_df['title'].isna()) & \n",
    "        (~combined_df['text'].isna())\n",
    "    ]\n",
    "    \n",
    "    # Drop duplicates based on title\n",
    "    combined_df = combined_df.drop_duplicates(subset=['title'])\n",
    "    \n",
    "    # Convert fake to int\n",
    "    combined_df['fake'] = combined_df['fake'].astype(int)\n",
    "    \n",
    "    return combined_df\n",
    "\n",
    "def preprocess_text(text, stop_words=set(stopwords.words('english'))):\n",
    "    # Tokenization\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    \n",
    "    # Remove stopwords and lemmatize\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens if token.isalnum() and token not in stop_words]\n",
    "    \n",
    "    return ' '.join(tokens)\n",
    "\n",
    "def remove_length_outliers(df, threshold_percentile=95):\n",
    "   length_threshold = df['text_length'].quantile(threshold_percentile/100)\n",
    "   df_filtered = df[df['text_length'] <= length_threshold].copy()\n",
    "   print(f\"{len(df) - len(df_filtered)} articles > {length_threshold} chars\")\n",
    "   return df_filtered\n",
    "\n",
    "def preprocess_dataset(df):\n",
    "    # Clean text\n",
    "    df['title_clean'] = df['title'].apply(preprocess_text)\n",
    "    df['text_clean'] = df['text'].apply(preprocess_text)\n",
    "    \n",
    "    # Additional features\n",
    "    df['title_length'] = df['title'].str.len()\n",
    "    df['text_length'] = df['text'].str.len()\n",
    "    \n",
    "    # Remove outliers\n",
    "    df = remove_length_outliers(df)\n",
    "    \n",
    "    return df\n",
    "\n",
    "df = combine_news_datasets('../data/Fake.csv', '../data/True.csv', '../data/final_en.csv', '../data/WELFake_Dataset.csv')\n",
    "df = preprocess_dataset(df)\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b9a0579-7c8d-47e5-b7f4-8be91ac410a9",
   "metadata": {},
   "source": [
    "# Fake News Detection Model Pipeline\n",
    "\n",
    "## Architecture\n",
    "- **Feature Engineering**\n",
    "  - Title TFIDF (20 features, unigrams & bigrams)\n",
    "  - Text TFIDF (50 features, unigrams & bigrams)\n",
    "  - Custom TextSelector for DataFrame column handling\n",
    "  \n",
    "## Models Evaluated\n",
    "| Model | Accuracy | Std Dev |\n",
    "|-------|----------|---------|\n",
    "| Logistic Regression | 0.893 | 0.185 |\n",
    "| Naive Bayes | 0.801 | 0.186 |\n",
    "| Random Forest | 0.900 | 0.216 |\n",
    "\n",
    "## Model Selection\n",
    "While Random Forest achieved marginally better accuracy (0.900 vs 0.893), we opt for Logistic Regression because:\n",
    "- Similar performance with lower complexity\n",
    "- Better interpretability\n",
    "- Faster training and inference\n",
    "- Lower variance (0.185 vs 0.216)\n",
    "\n",
    "The 0.7% accuracy improvement from Random Forest doesn't justify its added complexity and computational overhead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "18eb53af-bda2-4832-b673-09277fc603b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Logistic Regression:\n",
      "Mean accuracy: 0.893 (+/- 0.185)\n",
      "\n",
      "Naive Bayes:\n",
      "Mean accuracy: 0.801 (+/- 0.186)\n",
      "\n",
      "Random Forest:\n",
      "Mean accuracy: 0.900 (+/- 0.216)\n",
      "\n",
      "Best model: Random Forest with accuracy: 0.900\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "class TextSelector:\n",
    "    def __init__(self, field):\n",
    "        self.field = field\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return X[self.field]\n",
    "\n",
    "def create_features():\n",
    "    combined_features = FeatureUnion([\n",
    "        ('title', Pipeline([\n",
    "            ('selector', TextSelector('title_clean')),\n",
    "            ('tfidf', TfidfVectorizer(max_features=20, ngram_range=(1, 2)))\n",
    "        ])),\n",
    "        ('text', Pipeline([\n",
    "            ('selector', TextSelector('text_clean')),\n",
    "            ('tfidf', TfidfVectorizer(max_features=50, ngram_range=(1, 2)))\n",
    "        ])),\n",
    "    ])\n",
    "    \n",
    "    return combined_features\n",
    "\n",
    "\n",
    "def evaluate_models(df):\n",
    "    features = create_features()\n",
    "    \n",
    "    models = {\n",
    "        'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),\n",
    "        'Naive Bayes': MultinomialNB(),\n",
    "        'Random Forest': RandomForestClassifier(n_estimators=120, random_state=42)\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    for name, model in models.items():\n",
    "        pipeline = Pipeline([\n",
    "            ('features', features),\n",
    "            ('classifier', model)\n",
    "        ])\n",
    "        \n",
    "        scores = cross_val_score(\n",
    "            pipeline, \n",
    "            df,\n",
    "            df['fake'], \n",
    "            cv=5, \n",
    "            scoring='accuracy',\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        \n",
    "        results[name] = {\n",
    "            'mean_accuracy': scores.mean(),\n",
    "            'std_accuracy': scores.std()\n",
    "        }\n",
    "        \n",
    "        print(f\"\\n{name}:\")\n",
    "        print(f\"Mean accuracy: {scores.mean():.3f} (+/- {scores.std() * 2:.3f})\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run evaluation\n",
    "results = evaluate_models(df)\n",
    "\n",
    "# Find best model\n",
    "best_model = max(results.items(), key=lambda x: x[1]['mean_accuracy'])\n",
    "print(f\"\\nBest model: {best_model[0]} with accuracy: {best_model[1]['mean_accuracy']:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
